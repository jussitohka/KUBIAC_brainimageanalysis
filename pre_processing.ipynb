{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65cd53a0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# MRI analysis pipeline\n",
    "\n",
    "<img src=\"pipeline.png\" width=\"900\" />\n",
    "\n",
    "# MRI pre-processing\n",
    "## fMRI pre-processing pipeline\n",
    "\n",
    "<img src=\"fMRI_pipeline.png\" width=\"900\" />\n",
    "\n",
    "## dMRI pre-processing pipeline\n",
    "\n",
    "<img src=\"dMRI_pipeline.png\" width=\"700\" />\n",
    "\n",
    "## Co-ordinate systems\n",
    "### 3D Cartesian\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/69/Coord_system_CA_0.svg\" width=\"400\" />\n",
    "\n",
    "### 3D Spherical\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/4f/3D_Spherical.svg\" width=\"400\" />\n",
    "\n",
    "## Inter-subject co-registration\n",
    "\n",
    "We would like to put two 3D images from two different subjects on top of each other as well as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd599a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"560\" height=\"560\" controls loop>\n",
       "  <source src=\"before_coreg.mov\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two images before co-registration\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"560\" height=\"560\" controls loop>\n",
    "  <source src=\"before_coreg.mov\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ee3fa18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"560\" height=\"560\" controls loop>\n",
       "  <source src=\"after_coreg.mov\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two images after co-registration\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"560\" height=\"560\" controls loop>\n",
    "  <source src=\"after_coreg.mov\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c9b734",
   "metadata": {},
   "source": [
    "A standard way to do this is to use one of the brain images as a registration target and move the other into its reference frame. This is typically accomplished in two phases: 1) linear, and 2) non-linear co-registration.\n",
    "\n",
    "### Linear transformations\n",
    "\n",
    "All of the linear transformations are global, in a way that they affect each voxel the same way, and can be descrbed using a 4x4 transformation matrix,\n",
    "\n",
    "$$\n",
    "T =\n",
    "\\begin{pmatrix}\n",
    "   0.1 & 0   & 0   & -5   \\\\\n",
    "   0   & 0.1 & 0   & -6.7 \\\\\n",
    "   0   & 0   & 0.1 & -10  \\\\\n",
    "   0   & 0   & 0   & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This example matrix describes a space with isotropic voxel size (of 0.1 units) and a translation of -5 -6.7 -10 units (usually in x,y,z coordinates). The linear trasformations can be split into translation, rotation, scaling, shear, and perspective transformations.\n",
    "\n",
    "\n",
    "Original image\n",
    "\n",
    "<img src=\"orig_img.png\" width=\"500\" />\n",
    "\n",
    "Translation\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"trans_img.png\" width=\"500\" /> </td>\n",
    "<td> \n",
    "    $$\n",
    "T_{trans} =\n",
    "\\begin{pmatrix}\n",
    "   1 & 0 & 30 \\\\\n",
    "   0 & 1 & 30 \\\\\n",
    "   0 & 0 & 1  \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Scaling\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"scaled_img.png\" width=\"500\" /> </td>\n",
    "<td> \n",
    "$$\n",
    "T_{scaling} =\n",
    "\\begin{pmatrix}\n",
    "   1.3 & 0   & 10 \\\\\n",
    "   0   & 0.8 & 30 \\\\\n",
    "   0   & 0   & 1  \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Rotation\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"rot_img.png\" width=\"500\" /> </td>\n",
    "<td> \n",
    "$$\n",
    "T_{rot} =\n",
    "\\begin{pmatrix}\n",
    "   \\cos(\\theta) & -\\sin(\\theta) & 30 \\\\\n",
    "   \\sin(\\theta) &  \\cos(\\theta) & 30 \\\\\n",
    "   0            & 0             & 1  \n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "   0.980 & -0.199 & 40 \\\\\n",
    "   0.199 &  0.980 & 20 \\\\\n",
    "   0     &  0     & 1  \n",
    "\\end{pmatrix}\n",
    ",\n",
    "$$\n",
    "$$ \\theta = .2 $$\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Shear\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"sheared_img.png\" width=\"500\" /> </td>\n",
    "<td> \n",
    "$$\n",
    "T_{shear} =\n",
    "\\begin{pmatrix}\n",
    "   1   & 0.1 & 10 \\\\\n",
    "   0.1 & 1   & 30 \\\\\n",
    "   0   & 0   & 1  \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Perspective\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"perspective_img.png\" width=\"500\" /> </td>\n",
    "<td> \n",
    "$$\n",
    "T_{perspective} =\n",
    "\\begin{pmatrix}\n",
    "   1     & 0 & 10 \\\\\n",
    "   0     & 1 & 30 \\\\\n",
    "   0.001 & 0 & 1  \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "The perspective transformation does not happen in MRI data, so the last row of T is zeros apart from the diagonal 1.\n",
    "\n",
    "#### How to use the matrices\n",
    "\n",
    "Each voxel location in the 3D image is represented with a vector in a cartesian coordinate system. A mesh. The transformed vector is a result of matrix-vector multiplication\n",
    "\n",
    "\\begin{gather}\n",
    "  v_{old} =\n",
    "  \\begin{pmatrix}\n",
    "      x\\\\\n",
    "      y\\\\\n",
    "      z\n",
    "  \\end{pmatrix}\n",
    "  \\\\\n",
    "  v_{new} = T v_{old}\n",
    "\\end{gather}\n",
    "\n",
    "Several transformations can be applied to the same vector, which may be useful in longitudinal studies, particularly the ones with progressive deformations.\n",
    "\n",
    "![longtudinal](longitudinal_coregistration.png)\n",
    "\n",
    "$$ v_{new} = T_1 T_2 T_3 T_4 v_{old}$$\n",
    "\n",
    "\n",
    "#### Additional terminology\n",
    "\n",
    "Linear co-registration is often divided into two separate parts:\n",
    "    1. rigid co-registration, where the object experiences translations and rotations but remains intact otherwise; requiring 6 parameters, 3 translation, 3 rotation\n",
    "    2. affine co-registration, where - 9 or 12 parameters (figure)\n",
    "\n",
    "\n",
    "### Non-linear co-registration\n",
    "\n",
    "* talk about the liberties added compared to the linear case\n",
    "\n",
    "Non-linear registration performs local transformations. It is used primarily to correct for local deformations. As unconstrained non-linear transform can fit anything to everything, we need constraints. One could approximate that deformations are linear in the smaller scale, and use piecewise linear co-registration to correct for deformations. Or, that objects close to each other in the original image should be relatively close in the transformed image, too. The latest is implemented in ANTs software as diffeomorphic mapping.\n",
    "\n",
    "```shell\n",
    "antsRegistration --dimensionality 3 --float 0 --output [trnsfrm_nl_,deformed.nii.gz,inverse.nii.gz] --interpolation Linear --winsorize-image-intensities [0.005,0.995] --use-histogram-matching 0 --initial-moving-transform trnsfrm_rigid_0GenericAffine.mat --transform SyN[0.2,2,0] --metric CC[$(SIGMA_FILE),orig.nii.gz,1,4] --convergence [400x300x200,1e-5,10] --shrink-factors 4x2x1 --smoothing-sigmas 2x1x0vox --verbose\n",
    "```\n",
    "\n",
    "The product is often a deformation vector field, which describes local deformations of the original image; where to move the voxel center in order to produce the deformed image.\n",
    "\n",
    "## Metrics\n",
    "\n",
    "\n",
    "Correlation: \n",
    "$$\n",
    "r =\\frac{n \\sum{xy} - (\\sum{x})(\\sum{y})}{\\sqrt{\\left(n \\sum{x^2}-(\\sum{x})^2\\right)\\left(n \\sum{y^2}-(\\sum{y})^2\\right)}}\n",
    "$$\n",
    "\n",
    "How to describe a relationship between two variables: Linear relationship: fit a line so that the fit minimizes the sum of distances between the points and the line \n",
    "\n",
    "\n",
    "<img src=\"PNS.png\" width=\"350\" />(PNS.png)\n",
    "\n",
    "    * cross correlation: move one image and find the minimal correlation\n",
    "    * show two brain images partially on top of each other and and nicely registered + scatter plots from both\n",
    "\n",
    "* cross correlation - same to same - show images and scatter plots\n",
    "    * show two brain images partially on top of each other and nicely registered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd1a3426",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"560\" height=\"560\" controls loop>\n",
       "  <source src=\"before_coreg.mov\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two images before co-registration\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"560\" height=\"560\" controls loop>\n",
    "  <source src=\"before_coreg.mov\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260071bb",
   "metadata": {},
   "source": [
    "<img src=\"orig_scatter.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a49ad60",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"560\" height=\"560\" controls loop>\n",
       "  <source src=\"after_coreg.mov\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two images after co-registration\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"560\" height=\"560\" controls loop>\n",
    "  <source src=\"after_coreg.mov\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a429cd",
   "metadata": {},
   "source": [
    "<img src=\"coreg_scatter.png\" width=\"300\" />\n",
    "<img src=\"both_scatter.png\" width=\"300\" />\n",
    "\n",
    "    * talk about correlating and non-correlating data using the scatters\n",
    "    * maybe brain + ct?\n",
    "    * and then the solution\n",
    "\n",
    "\n",
    "* mutual information - different to different\n",
    "    * show images, talk about common information\n",
    "    * show scatterplot -> no correlation, show joint information scatter -> correlation\n",
    "\n",
    "$$\n",
    "\\mathrm {I}(X;Y) = \\sum_{y \\in Y}{\\sum_{x \\in X}{p_{(X,Y)}(x,y) \\log{\\left(\\frac{p_{(X,Y)}(x,y)}{p_X(x) p_Y(y)}\\right)}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_{X,Y}(x,y) = p (X=x\\ \\mathrm {and} \\ Y=y)\n",
    "$$\t\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/95/Multivariate_normal_sample.svg\" />\n",
    "</div>\n",
    "\n",
    "* how does this simplify to intra-subject?\n",
    "     * only rigid needed?\n",
    "\n",
    "\n",
    "### Interpolation\n",
    "\n",
    "* voxel w values - nearest neigh, linear, etc\n",
    "\n",
    "### Additional from co-reg\n",
    "\n",
    "Additional properties of the co-registration that we might want\n",
    "* also would like to perform transformations backwards without computing the transforms again\n",
    "* A mask in the target that one wants to place on the original individual brain image\n",
    "    * 4x4 matrix inverse gives the inverse linear transformation\n",
    "    \n",
    "$$ v_{old} = T^{-1} v_{new} $$\n",
    "    \n",
    "    * non-linear mapping is not that easy to invert? -> some mechanism needed\n",
    "        * one good way to do it is to estimage both directions simultaneously\n",
    "\n",
    "\n",
    "## (time-)series preprocessing\n",
    "\n",
    "Talk about how this is just a more simple application of the previous tools.\n",
    "\n",
    "We have acquired a series of 3D volumes\n",
    "In fMRI, the time-related contrast comes from blood oxygen-induced changes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b00a4423",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"640\" controls>\n",
       "  <source src=\"fmri.mov\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"640\" controls>\n",
    "  <source src=\"fmri.mov\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a665e2",
   "metadata": {},
   "source": [
    "\n",
    "In dMRI, the contrast comes from diffusion sensitizing gradient-induced differences in the volumes. Signal in a gradient-specified direction is measured, the signal attenuates (disappears) faster in the directions where water molecules can diffuse more. And this gives an orientation selective contrast.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "658122b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"640\" controls>\n",
       "  <source src=\"dwi.mov\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"640\" controls>\n",
    "  <source src=\"dwi.mov\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1c107f",
   "metadata": {},
   "source": [
    "Same brain, contrast etc: rigid co-registration except for susceptibility-induced artefacts or distance-to-the-receiver-related field inhomogeneities\n",
    "    \n",
    "fMRI & dMRI\n",
    "        * show in video\n",
    "* if the series contain subject motion, voxel (time-)series are not reliable\n",
    "    * motion correction (with references to co-reg; rigid w CC-type metric)\n",
    "        * rigid co-reg if no acquisition-induced distortions\n",
    "            * video of pre and post\n",
    "        * fMRI specific: slice-timing\n",
    "            * 2D acquisitions - if slices are \n",
    "        * dMRI specific: eddy current correction\n",
    "            * gradient changes induce eddy currents that can be corrected with specialiced algorigthms\n",
    "        * any way to show the distortions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07ae00a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"640\" controls>\n",
       "  <source src=\"mc_fmri.mov\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"640\" controls>\n",
    "  <source src=\"mc_fmri.mov\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c4ea6b",
   "metadata": {},
   "source": [
    "## Spatial de-noising\n",
    "\n",
    "dti enemmän\n",
    "\n",
    "## Smoothing\n",
    "\n",
    "Smoothing is performed for several reasons\n",
    "\n",
    "Some co-registration algorightms work better with smoothed data\n",
    "\n",
    "Smoothing removes small variations in multi-subject studies\n",
    "\n",
    "Data becomes more gaussian after smoothing, which helps in statistical inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "422acbd9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"640\" controls>\n",
       "  <source src=\"smoothing.mov\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"640\" controls>\n",
    "  <source src=\"smoothing.mov\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cced04",
   "metadata": {},
   "source": [
    "\n",
    "How does the process look like (when using the tools presented earlier)\n",
    "\n",
    "<img src=\"fMRI_pipeline.png\" width=\"900\" />\n",
    "\n",
    "<img src=\"dMRI_pipeline.png\" width=\"700\" />\n",
    "\n",
    "\n",
    "An example code in Snakemake formalism performing slice timing and motion correction to a dataset ../derivatives/{dataset}_EPI_ms.nii.gz, where {dataset} stands for any string. Slice timing is performed using FSL tool slicetimer and motion correction with ANTs tool antsMotionCorr.\n",
    "\n",
    "```python\n",
    "rule motion_correction:\n",
    "    input:\n",
    "        data=\"../derivatives/{dataset}_EPI_ms_st.nii.gz\",\n",
    "        reference=\"../derivatives/{dataset}_EPI_ms_st_reference.nii.gz\"\n",
    "    output:\n",
    "        \"../derivatives/{dataset}_EPI_ms_st_mc.nii.gz\"\n",
    "    params:\n",
    "        prefix=\"../derivatives/{dataset}_EPI_st\"\n",
    "    shell:\n",
    "        \"antsMotionCorr --dimensionality 3 --output [{params.prefix}_mc_,{output},{params.prefix}_ref.nii.gz] --transform Rigid[0.1] --metric GC[{input.reference},{input.data},1,32,Regular,0.2] --iterations 50x50 --useFixedReferenceImage 1 --useScalesEstimator 1 --smoothingSigmas 2x0 --shrinkFactors 2x1\"\n",
    "\n",
    "rule slice_timing:\n",
    "    input:\n",
    "        \"../derivatives/{dataset}_EPI_ms.nii.gz\"\n",
    "    output:\n",
    "        \"../derivatives/{dataset}_EPI_ms_st.nii.gz\"\n",
    "    shell:\n",
    "        \"slicetimer -i {input} -o {output} -r 1 --odd\"\n",
    "```\n",
    "\n",
    "* show with images & videos\n",
    "* voxel time-series before and after the process fig\n",
    "* työkaluja löytyy\n",
    "    * spesifisiä tai yleisiä\n",
    "    * osassa graafiset käyttöliittymät\n",
    "    * lista tähän\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "233d75d4",
   "metadata": {},
   "source": [
    "## Parametric maps from the data\n",
    "\n",
    "### Diffusion tensor imaging (DTI)\n",
    "\n",
    "The diffusion signal from the different gradient-weighted volumes is fitted into the 6-parameter exponential decay model\n",
    "\n",
    "$$ S(g,b) = S_0 e^{-bg^TDg}, $$\n",
    "where\n",
    "\n",
    "$$\n",
    "D =\n",
    "\\begin{pmatrix}\n",
    "   D_{xx} & D_{xy} & D_{xz} \\\\\n",
    "   D_{yx} & D_{yy} & D_{yz} \\\\\n",
    "   D_{zx} & D_{zy} & D_{zz} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "is the diffusion tensor, which can be visualized as an ellipsoid\n",
    "\n",
    "<img src=\"ellipsoid.png\" width=\"400\" />,\n",
    "\n",
    "where the eigenvalues of the rank-2 matrix can be used to compute diffusivity maps, such as Fractional Anisotropy FA\n",
    "\n",
    "$$ \\mathrm {FA} = \\sqrt{.5 \\frac{(\\lambda_1-\\lambda_2)^2 + (\\lambda_1-\\lambda_3)^2 + (\\lambda_2-\\lambda_3)^2}{\\lambda_1^2 + \\lambda_2^2 + \\lambda_3^2}} $$\n",
    "\n",
    "![FA](dwi_FA.png)\n",
    "\n",
    "or Mean Diffusivity\n",
    "\n",
    "![MD](dwi_MD.png)\n",
    "\n",
    "    * eigen decomposition -> eigenvalues and vectors\n",
    "    * eigenvalues -> parametric maps FA, MD show these\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8f864a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "* T1 T1 - rekot\n",
    "\n",
    "![T1](t1_img.png)\n",
    "![flair](flair_img.png)\n",
    "![flair_c](a_flair_img.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc0bce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
