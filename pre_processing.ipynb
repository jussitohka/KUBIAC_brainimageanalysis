{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65cd53a0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# MRI analysis pipeline\n",
    "\n",
    "<img src=\"pipeline.png\" width=\"900\" />\n",
    "\n",
    "# MRI pre-processing\n",
    "\n",
    "## Co-ordinate systems\n",
    "### 3D Cartesian\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/69/Coord_system_CA_0.svg\" width=\"400\" />\n",
    "\n",
    "### 3D Spherical\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/4f/3D_Spherical.svg\" width=\"400\" />\n",
    "\n",
    "## Inter-subject co-registration\n",
    "\n",
    "In order to use smallest possible regions of interest, voxels, we would like to put two 3D images from two different subjects on top of each other as well as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd599a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"560\" height=\"560\" controls loop>\n",
       "  <source src=\"human_t1.mov\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two images before co-registration\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"560\" height=\"560\" controls loop>\n",
    "  <source src=\"human_t1.mov\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c9b734",
   "metadata": {},
   "source": [
    "A standard way to do this is to use one of the brain images as a registration target and move the other into its reference frame. This is typically accomplished in two phases: 1) linear, and 2) non-linear co-registration.\n",
    "\n",
    "### Linear transformations\n",
    "\n",
    "All of the linear transformations are global, in a way that they affect each voxel the same way, and can be described using a 4x4 transformation matrix,\n",
    "\n",
    "$$\n",
    "T =\n",
    "\\begin{pmatrix}\n",
    "   0.1 & 0   & 0   & -5   \\\\\n",
    "   0   & 0.1 & 0   & -6.7 \\\\\n",
    "   0   & 0   & 0.1 & -10  \\\\\n",
    "   0   & 0   & 0   & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This example matrix describes a space with isotropic voxel size (of 0.1 units) and a translation of -5 -6.7 -10 units (usually in x,y,z coordinates). The linear trasformations can be split into translation, rotation, scaling, shear, and perspective transformations.\n",
    "\n",
    "\n",
    "Original image\n",
    "\n",
    "<img src=\"orig_img.png\" width=\"500\" />\n",
    "\n",
    "Translation\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"trans_img.png\" width=\"500\" /> </td>\n",
    "<td> \n",
    "    $$\n",
    "T_{trans} =\n",
    "\\begin{pmatrix}\n",
    "   1 & 0 & 30 \\\\\n",
    "   0 & 1 & 30 \\\\\n",
    "   0 & 0 & 1  \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Scaling\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"scaled_img.png\" width=\"500\" /> </td>\n",
    "<td> \n",
    "$$\n",
    "T_{scaling} =\n",
    "\\begin{pmatrix}\n",
    "   1.3 & 0   & 10 \\\\\n",
    "   0   & 0.8 & 30 \\\\\n",
    "   0   & 0   & 1  \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Rotation\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"rot_img.png\" width=\"500\" /> </td>\n",
    "<td> \n",
    "$$\n",
    "T_{rot} =\n",
    "\\begin{pmatrix}\n",
    "   \\cos(\\theta) & -\\sin(\\theta) & 30 \\\\\n",
    "   \\sin(\\theta) &  \\cos(\\theta) & 30 \\\\\n",
    "   0            & 0             & 1  \n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "   0.980 & -0.199 & 40 \\\\\n",
    "   0.199 &  0.980 & 20 \\\\\n",
    "   0     &  0     & 1  \n",
    "\\end{pmatrix}\n",
    ",\n",
    "$$\n",
    "$$ \\theta = .2 $$\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Shear\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"sheared_img.png\" width=\"500\" /> </td>\n",
    "<td> \n",
    "$$\n",
    "T_{shear} =\n",
    "\\begin{pmatrix}\n",
    "   1   & 0.1 & 10 \\\\\n",
    "   0.1 & 1   & 30 \\\\\n",
    "   0   & 0   & 1  \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Perspective\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"perspective_img.png\" width=\"500\" /> </td>\n",
    "<td> \n",
    "$$\n",
    "T_{perspective} =\n",
    "\\begin{pmatrix}\n",
    "   1     & 0 & 10 \\\\\n",
    "   0     & 1 & 30 \\\\\n",
    "   0.001 & 0 & 1  \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "The perspective transformation does not happen in MRI data, so the last row of T is zeros apart from the diagonal 1.\n",
    "\n",
    "#### How to use the matrices\n",
    "\n",
    "Each voxel location in the 3D image is represented with a vector in a cartesian coordinate system. The transformed vector is a result of matrix-vector multiplication\n",
    "\n",
    "\\begin{gather}\n",
    "  v_{old} =\n",
    "  \\begin{pmatrix}\n",
    "      x\\\\\n",
    "      y\\\\\n",
    "      z\n",
    "  \\end{pmatrix}\n",
    "  \\\\\n",
    "  v_{new} = T v_{old}\n",
    "\\end{gather}\n",
    "\n",
    "Several transformations can be applied to the same vector, which may be useful in longitudinal studies, particularly the ones with progressive deformations.\n",
    "\n",
    "<img src=\"longitudinal_coregistration.png\" width=\"900\" />\n",
    "\n",
    "$$ v_{new} = T_1 T_2 T_3 T_4 v_{old}$$\n",
    "\n",
    "\n",
    "#### Additional terminology\n",
    "\n",
    "Linear co-registration is often divided into two separate parts:\n",
    "\n",
    "* rigid co-registration, where the object experiences translations and rotations but remains intact otherwise; requiring 6 parameters (3 translation, 3 rotation)\n",
    "* affine co-registration, using 9 or 12 parameters\n",
    "\n",
    "\n",
    "### Non-linear co-registration\n",
    "\n",
    "Non-linear registration is used typically to correct for local deformations after a linear co-registration phase. As unconstrained non-linear transform can fit anything to everything, we need constraints. One way is to assume that deformations are linear in the smaller scale, and use piecewise linear co-registration to correct for deformations. Or, that objects close to each other in the original image should be relatively close in the transformed image, too. The latest is implemented in ANTs software as diffeomorphic mapping.\n",
    "\n",
    "```shell\n",
    "\n",
    "antsRegistration --dimensionality 3 --float 0 --output [trnsfrm_nl_,deformed.nii.gz,inverse.nii.gz] --interpolation Linear --winsorize-image-intensities [0.005,0.995] --use-histogram-matching 0 --initial-moving-transform trnsfrm_rigid_0GenericAffine.mat --transform SyN[0.2,2,0] --metric CC[target.nii.gz,orig.nii.gz,1,4] --convergence [400x300x200,1e-5,10] --shrink-factors 4x2x1 --smoothing-sigmas 2x1x0vox --verbose\n",
    "\n",
    "```\n",
    "\n",
    "The product of non-linear registration is often a deformation vector field, which describes local deformations of the original image; where to move the voxel center in order to produce the deformed image.\n",
    "\n",
    "## Co-registration metrics\n",
    "\n",
    "In order to perform co-registration, we need a way to measure goodness of registration. Correlation \n",
    "\n",
    "$$\n",
    "r =\\frac{n \\sum{xy} - (\\sum{x})(\\sum{y})}{\\sqrt{\\left(n \\sum{x^2}-(\\sum{x})^2\\right)\\left(n \\sum{y^2}-(\\sum{y})^2\\right)}}\n",
    "$$\n",
    "\n",
    "is one possible choice. The point of corrrelation is to fit a line so that the fit minimizes the sum of distances between the points and the line \n",
    "\n",
    "\n",
    "<img src=\"PNS.png\" width=\"400\" />\n",
    "\n",
    "and the correlation coefficient r tells the goodness of co-registration. A different solution is found by moving one of the images and computing correlation again. With the metric available, the co-registration task can be represented as a minimization problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd1a3426",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"560\" height=\"560\" controls loop>\n",
       "  <source src=\"before_coreg.mov\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two images before co-registration\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"560\" height=\"560\" controls loop>\n",
    "  <source src=\"before_coreg.mov\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260071bb",
   "metadata": {},
   "source": [
    "<img src=\"orig_scatter.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a49ad60",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"560\" height=\"560\" controls loop>\n",
       "  <source src=\"after_coreg.mov\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two images after co-registration\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"560\" height=\"560\" controls loop>\n",
    "  <source src=\"after_coreg.mov\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a429cd",
   "metadata": {},
   "source": [
    "<img src=\"coreg_scatter.png\" width=\"600\" />\n",
    "<img src=\"both_scatter.png\" width=\"600\" />\n",
    "\n",
    "Correlation metric works for data which correlates positively but may fail with negatively correlating data, such as when co-registering MRI and CT images to each other.\n",
    "\n",
    "A partial solution to the problem is mutual information metric\n",
    "\n",
    "$$\n",
    "\\mathrm {I}(X;Y) = \\sum_{y \\in Y}{\\sum_{x \\in X}{p_{(X,Y)}(x,y) \\log{\\left(\\frac{p_{(X,Y)}(x,y)}{p_X(x) p_Y(y)}\\right)}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_{X,Y}(x,y) = p (X=x\\ \\mathrm {and} \\ Y=y)\n",
    "$$\t\n",
    "\n",
    "which computes a joint probability distribution \n",
    "\n",
    "<div>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/95/Multivariate_normal_sample.svg\" />\n",
    "</div>\n",
    "\n",
    "and uses information theoretic approach to find common information in both of the images.\n",
    "\n",
    "### Interpolation\n",
    "\n",
    "After moving an image to another location, the image intensity values need to be recomputed in the new location and potentially new co-ordinate system. This process is called interpolation. Some common forms of interpolation are nearest neighbor, linear, quadratic, etc.\n",
    "\n",
    "In the nearest neighbor interpolation, the new grid voxel gets the value from the nearest image point. In the linear case, the value is a weighted sum of a linear fit using 8 nearest image voxels, where the weight is inversely proportional to the distance.\n",
    "\n",
    "<img src=\"linear_interpolation.png\" width=500 />\n",
    "\n",
    "The number of datapoints needed for interpolation rises dramatically with the number of dimensions as well as the complexity of the interpolating function. The internal interpolation has to be carefully chosen in order to get a smooth optimization problem as well as fast execution of the process.\n",
    "\n",
    "\n",
    "### Additional need for co-registration\n",
    "\n",
    "One might want to use the transforms the opposite way, for example to move a mask drawn on the target on the individual brain image. The linear transform matrix inverse provides the opposite transform\n",
    "    \n",
    "$$ v_{old} = T^{-1} v_{new} $$\n",
    "\n",
    "In the non-linear mapping, this is not as easy as the deformation field is discrete and does not provide direct opposite mapping for the inverse process. The best available method is to simultaneously estimate the non-linear trasformation both ways.\n",
    "\n",
    "## Time-series pre-processing\n",
    "\n",
    "In diffusion MRI (dMRI) and functional MRI (fMRI) we acquire series of 3D volumes.\n",
    "\n",
    "In dMRI, the contrast comes from diffusion sensitizing gradient-induced differences in the volumes. Signal in a gradient-specified direction is measured, the signal attenuates (disappears) faster in the directions where water molecules can diffuse more. And this gives an orientation selective contrast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b00a4423",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"640\" controls loop>\n",
       "  <source src=\"dwi.mov\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"640\" controls loop>\n",
    "  <source src=\"dwi.mov\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a665e2",
   "metadata": {},
   "source": [
    " \n",
    "In fMRI, the time-related contrast comes from blood oxygen-induced changes that are brought up by energy demand in a specific brain location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "658122b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"640\" controls loop>\n",
       "  <source src=\"fmri.mov\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"640\" controls loop>\n",
    "  <source src=\"fmri.mov\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1c107f",
   "metadata": {},
   "source": [
    "If the series contain subject motion, the voxel time-series do not represent real voxel signal. To correct for motion, we use co-registration tools presented above to transform each 3D volume of the series to the common reference frame. As the brain remains the same throughout the series acquisition rigid co-registration is often feasible for motion correction, except for potential susceptibility-induced artefacts or distance-to-the-receiver-related field inhomogeneities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07ae00a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"640\" controls>\n",
       "  <source src=\"mc_fmri.mov\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"640\" controls>\n",
    "  <source src=\"mc_fmri.mov\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c4ea6b",
   "metadata": {},
   "source": [
    "Each 3D fMRI volume is typically acquired as a series of 2D slices. The slice acquisitions are spread over the typical 1-2 seconds, over which the volume is acquired. As the assumption is that the signal is in the same time-point in each volume, this needs to be corrected.\n",
    "\n",
    "<img src=\"slice_timing.png\" width=900 />\n",
    "\n",
    "This slice-timing correction is performed with convolution, using timing of the slice acquisition.\n",
    "\n",
    "        * dMRI specific: eddy current correction\n",
    "            * gradient changes induce eddy currents that can be corrected with specialiced algorigthms\n",
    "\n",
    "## Smoothing\n",
    "\n",
    "Smoothing is performed for several reasons\n",
    "\n",
    "Some co-registration algorightms work better with smoothed data\n",
    "\n",
    "Smoothing removes small variations in multi-subject studies\n",
    "\n",
    "Data becomes more gaussian after smoothing, which helps in statistical inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "422acbd9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"640\" controls>\n",
       "  <source src=\"smoothing.mov\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"640\" controls>\n",
    "  <source src=\"smoothing.mov\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cced04",
   "metadata": {},
   "source": [
    "## The pre-processing pipeline\n",
    "\n",
    "The pre-processing steps are rather established and there exists plenty of software to perform the tasks reliably enough. Furthermore, the pre-processing steps can be automated with a bit of scripting releasing the researcher from the laborous manual tasks. Plenty of scripts exist that perform these tasks. \n",
    "\n",
    "fMRI:\n",
    "\n",
    "<img src=\"fMRI_pipeline.png\" width=\"900\" />\n",
    "\n",
    "dMRI:\n",
    "\n",
    "<img src=\"dMRI_pipeline.png\" width=\"700\" />\n",
    "\n",
    "\n",
    "An example code in Snakemake formalism performing slice timing and motion correction to an fMRI dataset ../derivatives/{dataset}_EPI_ms.nii.gz, where {dataset} stands for any string. Slice timing is performed using FSL tool slicetimer and motion correction with ANTs tool antsMotionCorr.\n",
    "\n",
    "```python\n",
    "rule motion_correction:\n",
    "    input:\n",
    "        data=\"../derivatives/{dataset}_EPI_ms_st.nii.gz\",\n",
    "        reference=\"../derivatives/{dataset}_EPI_ms_st_reference.nii.gz\"\n",
    "    output:\n",
    "        \"../derivatives/{dataset}_EPI_ms_st_mc.nii.gz\"\n",
    "    params:\n",
    "        prefix=\"../derivatives/{dataset}_EPI_st\"\n",
    "    shell:\n",
    "        \"antsMotionCorr --dimensionality 3 --output [{params.prefix}_mc_,{output},{params.prefix}_ref.nii.gz] --transform Rigid[0.1] --metric GC[{input.reference},{input.data},1,32,Regular,0.2] --iterations 50x50 --useFixedReferenceImage 1 --useScalesEstimator 1 --smoothingSigmas 2x0 --shrinkFactors 2x1\"\n",
    "\n",
    "rule slice_timing:\n",
    "    input:\n",
    "        \"../derivatives/{dataset}_EPI_ms.nii.gz\"\n",
    "    output:\n",
    "        \"../derivatives/{dataset}_EPI_ms_st.nii.gz\"\n",
    "    shell:\n",
    "        \"slicetimer -i {input} -o {output} -r 1 --odd\"\n",
    "```\n",
    "\n",
    "* show with images & videos\n",
    "* voxel time-series before and after the process fig\n",
    "* työkaluja löytyy\n",
    "    * spesifisiä tai yleisiä\n",
    "    * osassa graafiset käyttöliittymät\n",
    "    * lista tähän\n",
    "\n",
    "## Parametric maps from the data\n",
    "\n",
    "### Diffusion tensor imaging (DTI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70867e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"640\" controls loop>\n",
       "  <source src=\"dwi.mov\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"640\" controls loop>\n",
    "  <source src=\"dwi.mov\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233d75d4",
   "metadata": {},
   "source": [
    "\n",
    "The diffusion signal from the different gradient-weighted volumes is fitted into the 6-parameter exponential decay model\n",
    "\n",
    "$$ S(g,b) = S_0 e^{-b g^T Dg }, $$\n",
    "\n",
    "where b is the b-value, and g is the gradient direction, and\n",
    "\n",
    "$$\n",
    "D =\n",
    "\\begin{pmatrix}\n",
    "   D_{xx} & D_{xy} & D_{xz} \\\\\n",
    "   D_{yx} & D_{yy} & D_{yz} \\\\\n",
    "   D_{zx} & D_{zy} & D_{zz} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "is the diffusion tensor, which can be visualized as an ellipsoid\n",
    "\n",
    "<img src=\"ellipsoid.png\" width=\"400\" />\n",
    "\n",
    "where the eigenvalues of the rank-2 matrix can be used to compute diffusivity maps, such as Fractional Anisotropy FA\n",
    "\n",
    "$$ \\mathrm {FA} = \\sqrt{\\frac{1}{2} \\frac{(\\lambda_1-\\lambda_2)^2 + (\\lambda_1-\\lambda_3)^2 + (\\lambda_2-\\lambda_3)^2}{\\lambda_1^2 + \\lambda_2^2 + \\lambda_3^2}} $$\n",
    "\n",
    "<img src=\"dwi_FA.png\" width=\"650\" />\n",
    "\n",
    "or Mean Diffusivity\n",
    "\n",
    "$$ \\mathrm{MD} = (\\lambda_1 + \\lambda_2 + \\lambda_3)/3 $$\n",
    "\n",
    "<img src=\"dwi_MD.png\" width=\"650\" />\n",
    "\n",
    "\n",
    "    * eigen decomposition -> eigenvalues and vectors\n",
    "    * eigenvalues -> parametric maps FA, MD show these\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc0bce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
